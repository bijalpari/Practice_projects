{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7166f35",
   "metadata": {},
   "source": [
    "# LLM Model Ranking via Self-Generated Challenge Questions\n",
    "\n",
    "This notebook orchestrates a simple model-ranking workflow:\n",
    "- Ask one model to generate a challenging, nuanced question.\n",
    "- Pose that question to multiple models and collect their answers.\n",
    "- Compare answers to produce a rough ranking.\n",
    "\n",
    "Notes:\n",
    "- Keep prompts deterministic when possible to reduce variance.\n",
    "- Be mindful of API usage and costs.\n",
    "- Do not print full API keys; only partial, masked previews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# - os/json: environment and simple serialization\n",
    "# - dotenv: load API keys from .env\n",
    "# - openai: client for OpenAI-compatible APIs\n",
    "# - display utils: richer notebook output\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769308e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from a local .env file\n",
    "# override=True allows .env to replace existing env vars if needed\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found and begins sk-proj-\n",
      "Google API key found and begins AI\n"
     ]
    }
   ],
   "source": [
    "# Fetch API keys (masked preview only)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API key found and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API key not found\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API key found and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API key not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23265f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate a single challenging evaluation question\n",
    "request = (\n",
    "    \"Please come up with a challenging, nuanced question that I can ask a number of LLMs \"\n",
    "    \"to evaluate their intelligence. Answer only with the question, no explanation.\"\n",
    ")\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3d02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the message payload to be sent to the model\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model to generate the evaluation question\n",
    "# Note: ensure your OPENAI_API_KEY is set; adjust model name as needed\n",
    "openai = OpenAI\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946adbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare containers for competitor models and their answers\n",
    "competitors = []  # e.g., [(\"gpt-4o-mini\", clientA), (\"gemini-1.5-pro\", clientB)]\n",
    "answers = []\n",
    "\n",
    "# Re-seed the conversation with the generated question\n",
    "messages = [{'role': 'user', 'content': question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate `competitors` with the models/clients you want to compare.\n",
    "# The API we know well gpt-4o-mini\n",
    "# Query each model with the same `messages` and store the response text in `answers`.\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The API we know well emini-2.0-flash\n",
    "# Query each model with the same `messages` and store the response text in `answers`.\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad366c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(competitors)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip answers\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\"\n",
    "\n",
    "print(together)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cccc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdf7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3410ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f63c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "result_dict = json.loads(results)\n",
    "ranks = result_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print (f\"Rank {index+1}: {competitor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
